3台机器配置为2G2核心
主机名   操作系统版本     ip           docker version   kubeadm/kubelet/kubectl version
master    7.9.2009  192.168.189.200                        1.23.9
node1     7.9.2009  192.168.189.201                        1.23.9
node2     7.9.2009  192.168.189.202                        1.23.9
一、基础环境准备
1、修改主机名
[root@localhost ~]# hostnamectl set-hostname master
[root@localhost ~]# su
[root@master ~]#

2、关闭防火墙
[root@master ~]# setenforce 0
[root@master ~]# sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
[root@master ~]# systemctl disable --now firewalld

3、禁用swap
[root@master ~]# swapoff -a
[root@master ~]# sed -i.bak '/swap/s/^/#/g' /etc/fstab

4、配置dns
[root@master ~]# echo "nameserver 223.5.5.5" >> /etc/resolv.conf

5、配置yun源
[root@master ~]# cd /etc/yum.repos.d/
[root@master yum.repos.d]# curl -O   https://mirrors.aliyun.com/repo/Centos-7.repo
[root@master yum.repos.d]# curl -O https://mirrors.aliyun.com/repo/epel-7.repo
[root@master yum.repos.d]# curl -O https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@master yum.repos.d]# cat > kubernetes.repo  <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
[root@master yum.repos.d]# yum clean all && yum makecache
初次yum makecache时在生成centos源缓存时报错
base/7/x86_64/filelists_db     FAILED                                          
http://mirrors.aliyuncs.com/centos/7/os/x86_64/repodata/d6d94c7d406fe7ad4902a97104b39a0d8299451832a97f31d71653ba982c955b-filelists.sqlite.bz2:
 [Errno 14] curl#7 - "Failed connect to mirrors.aliyuncs.com:80; Connection refused"Trying other mirror.
查看Centos-7.repo文件为http，修改为https后再次执行makecahce正常
[root@master yum.repos.d]# sed -i.bak 's/http/https/g' Centos-7.repo
[root@master yum.repos.d]# yum clean all && yum makecache

6、下载常见安装包
[root@master ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 wegt net-tools vim sshpass

7、配置ssh互信
[root@master ~]# ssh-keygen 按两下空格
[root@master ~]# cat believe.sh
#!/bin/bash
pass=root.2020
awk 'NR>2{print $2}' /etc/hosts  |while read line
do
	sshpass -p $pass ssh-copy-id -o stricthostkeychecking=no $line
done
[root@master ~]# chmod +x believe.sh 
[root@master ~]# ./believe.sh 

8、配置时间同步
修改时区
[root@master ~]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
[root@master ~]# timedatectl set-timezone 'Asia/Shanghai'
修改chrony.conf文件
[root@master ~]# grep -Ev "^#|^$" /etc/chrony.conf
server cn.pool.ntp.org 
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.168.189.0/24
local stratum 10
logdir /var/log/chrony
拷贝给node节点
[root@master ~]# scp -rp /etc/chrony.conf node1:/etc/
chrony.conf                                                                                                 100% 1183   356.1KB/s   00:00    
[root@master ~]# scp -rp /etc/chrony.conf node2:/etc/
chrony.conf                                                                                                 100% 1183   373.9KB/s   00:00  
设置开机自启并启动chronyd服务，查看Time zone和NTP synchronized
[root@master ~]# systemctl enable --now chronyd
查看时区和时间同步
[root@master ~]# timedatectl 
      Local time: Mon 2022-08-22 10:05:27 CST
  Universal time: Mon 2022-08-22 02:05:27 UTC
        RTC time: Mon 2022-08-22 02:05:27
       Time zone: Asia/Shanghai (CST, +0800)
     NTP enabled: yes
NTP synchronized: yes
 RTC in local TZ: no
      DST active: n/a
[root@master ~]# hwclock -w

9、修改hosts文件
[root@master ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.189.201 node1
192.168.189.202 node2
[root@master ~]# scp -rp /etc/hosts node1:/etc/                                                                                                    100%  203    59.0KB/s   00:00    
[root@master ~]# scp -rp /etc/hosts node2:/etc/

10、配置内核参数
[root@master ~]# cat >>/etc/sysctl.conf <<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
vm.swappiness=0
EOF
[root@master ~]# sysctl -p
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
使用modprobe加载br_netfilter模块
[root@master ~]# modprobe br_netfilter
[root@master ~]# sysctl -p
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1

11、安装docker-ce
使用如下命令查看docker版本，本次选择
[root@master ~]# yum list docker-ce --showduplicates
[root@master ~]# yum install -y docker-ce-20.10.10
[root@master ~]# systemctl enable --now docker
[root@master ~]# cat >>/etc/docker/daemon.json <<EOF
{
  "registry-mirrors": ["https://v16stybc.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
[root@master ~]# systemctl daemon-reload
[root@master ~]# systemctl restart docker
使用如下命令查看Registry Mirrors和Cgroup Driver是否修改成功
[root@master ~]# docker info | grep -in1  reg
47- Debug Mode: false
48: Registry: https://index.docker.io/v1/
49- Labels:
50- Experimental: false
51: Insecure Registries:
52-  127.0.0.0/8
53: Registry Mirrors:
54-  https://u96790vf.mirror.aliyuncs.com/
[root@master ~]# docker info | grep -i dri
 Storage Driver: overlay2
 Logging Driver: json-file
 Cgroup Driver: systemd
使用docker运行hello-world测试
[root@master ~]# docker run hello-world

12、安装kubelet、kubeadm、kubectl
[root@master ~]# yum list kubelet --showduplicates
[root@master ~]# yum install -y kubeadm-1.17.12 kubectl-1.17.12 kubelet-1.17.12
[root@master ~]# systemctl enable --now kubelet
[root@master ~]# systemctl status kubelet ##此时查看kubelet没有启动属于正常现象，kubeadm init后就会启动
[root@master ~]# ss -tunlp | grep kubelet ##端口没有放开也属于正常现象，kubeadm init后端口会放开

13、kubelet命令补全
[root@master ~]# yum install -y bash-completion
[root@master ~]# source /etc/profile.d/bash_completion.sh 
[root@master ~]# echo "source <(kubectl completion bash)" >>~/.bash_profile 
[root@master ~]# source ~/.bash_profile

14、下载镜像
[root@master ~]# cat images.sh 
#!/bin/bash
url=registry.cn-hangzhou.aliyuncs.com/google_containers
version=v1.17.12
images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
for imagename in ${images[@]} ; do
  docker pull $url/$imagename
  docker tag $url/$imagename k8s.gcr.io/$imagename
  docker rmi -f $url/$imagename     #此处有问题，$url/$imagename后面必须跟上版本号才可以删除
done
[root@master ~]# chmod +x images.sh
[root@master ~]# ./images.sh 

15、kubeadm init
初始化可以使用两种方式，一种如下，还有一种创建配置文件，配置更详细kubeadm init --config kubeadm-config.yaml
cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1alpha2 
kind: MasterConfiguration 
kubernetesVersion: v1.17.2 
api:
    advertiseAddress: 192.168.189.200
    bindPort: 6443 
    controlPlaneEndpoint: "" 
imageRepository: k8s.gcr.io 
kubeProxy: 
    config: 
    mode: "ipvs"     
    ipvs: 
        ExcludeCIDRs: null 
        minSyncPeriod: 0s 
        scheduler: "" 
        syncPeriod: 30s 
kubeletConfiguration: 
    baseConfig: 
        cgroupDriver: systemd
        clusterDNS: 
        -10.96.0.10 
        clusterDomain: cluster.local 
        failSwapOn: false 
        resolvConf: /etc/resolv.conf 
        staticPodPath: /etc/kubernetes/manifests 
networking: 
    dnsDomain: cluster.local 
    podSubnet: 10.1.200.0/24
    serviceSubnet: 10.1.100.0/24
[root@master ~]# kubeadm init --apiserver-advertise-address=192.168.189.200 --image-repository registry.aliyuncs.com/google_containers --kuber
netes-version v1.17.12 --service-cidr=10.1.100.0/24 --pod-network-cidr=10.1.200.0/24
输出如下表示初始化成功
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.189.200:6443 --token yquoub.htqg9ff5za70wxa4 \
    --discovery-token-ca-cert-hash sha256:1897ee409743739e39d713e9da8ea99571eb578bbd163046fbf2b882f99f6e0d 
此时查看kubelet状态和端口开放情况
[root@master ~]# ss -tunlp | grep kubelet
tcp    LISTEN     0      128    127.0.0.1:40820                 *:*                   users:(("kubelet",pid=27544,fd=8))
tcp    LISTEN     0      128    127.0.0.1:10248                 *:*                   users:(("kubelet",pid=27544,fd=27))
tcp    LISTEN     0      128    [::]:10250              [::]:*                   users:(("kubelet",pid=27544,fd=24))
[root@master ~]# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Mon 2022-08-22 14:54:10 CST; 16min ago

16、加载环境变量
使用普通用户执行如下命令
[root@kube ~]# mkdir -p $HOME/.kube
[root@kube ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@kube ~]# chown $(id -u):$(id -g) $HOME/.kube/config
我使用的是root用户，只需要.bash_profile中添加环境变量就行
[root@master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >>~/.bash_profile 
[root@master ~]# source ~/.bash_profile

17、安装pod网络，本次使用的是flannel
[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
namespace/kube-flannel created
安装完pod网络后要等到几分钟，执行这个命令查看直到都为running状态
[root@master ~]# kubectl get pods -n kube-system

18、work节点加入集群
[root@node2 yum.repos.d]# yum install -y kubeadm-1.17.12 kubectl-1.17.12 kubelet-1.17.12
[root@node2 yum.repos.d]# yum install -y bash-completion
[root@node2 yum.repos.d]# source /etc/profile.d/bash_completion.sh 
[root@node2 yum.repos.d]# echo "source <(kubectl completion bash)" >>~/.bash_profile 
[root@node2 yum.repos.d]# systemctl enable --now kubelet ##此时查看kubelet没有启动属于正常现象，kubeadm join后就会启动
[root@node1 ~]# kubeadm join 192.168.189.200:6443 --token yquoub.htqg9ff5za70wxa4 \
>     --discovery-token-ca-cert-hash sha256:1897ee409743739e39d713e9da8ea99571eb578bbd163046fbf2b882f99f6e0d 

19、健康检查发现controller-manager、scheduler状态Unhealthy，查看master组件状态时出现错误
[root@master ~]# kubectl get cs
NAME                 STATUS      MESSAGE                                                                                     ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"}  
修改/etc/kubernetes/manifests/kube-controller-manager.yaml、/etc/kubernetes/manifests/kube-scheduler.yaml文件
[root@master manifests]# cd /etc/kubernetes/manifests
[root@master manifests]# ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
[root@master manifests]# cat kube-controller-manager.yaml | grep port
#    - --port=0
        port: 10257
[root@master manifests]# cat kube-scheduler.yaml | grep port
#    - --port=0
        port: 10259
修改完成后，重新apply即可，再次检查健康状态
[root@master manifests]# kubectl apply -f kube-scheduler.yaml 
pod/kube-scheduler created
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
[root@master manifests]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
[root@master manifests]# 

遇到的问题
使用如下命令查看kube-controller-manager状态为CrashLoopBackOff 
[root@master ~]# kubectl get pods -n kube-system
NAME                                    READY   STATUS              RESTARTS   AGE
coredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          3h41m
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          3h41m
etcd-master                             1/1     Running             1          3h41m
kube-apiserver-master                   1/1     Running             1          3h41m
kube-controller-manager                 0/1     CrashLoopBackOff    12         38m
kube-controller-manager-master          1/1     Running             0          42m
kube-proxy-dzr8s                        1/1     Running             1          3h41m
kube-proxy-vkqzq                        1/1     Running             0          141m
kube-proxy-vrw77                        1/1     Running             0          141m
kube-scheduler                          1/1     Running             0          38m
kube-scheduler-master                   1/1     Running             0          41m
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          93m
使用如下命令查看日志报错，找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master ~]# kubectl logs -n kube-system kube-controller-manager
I0822 10:34:23.104970       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory

使用如下命令查看kube-controller-manager运行在了node节点
[root@master ~]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS              RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS G
ATEScoredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          3h46m   <none>            master   <none>           <none>
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          3h46m   <none>            master   <none>           <none>
etcd-master                             1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-apiserver-master                   1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-controller-manager                 0/1     CrashLoopBackOff    13         43m     192.168.189.202   node2    <none>           <none>
kube-controller-manager-master          1/1     Running             0          47m     192.168.189.200   master   <none>           <none>
kube-proxy-dzr8s                        1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-proxy-vkqzq                        1/1     Running             0          147m    192.168.189.201   node1    <none>           <none>
kube-proxy-vrw77                        1/1     Running             0          146m    192.168.189.202   node2    <none>           <none>
kube-scheduler                          1/1     Running             0          44m     192.168.189.201   node1    <none>           <none>
kube-scheduler-master                   1/1     Running             0          46m     192.168.189.200   master   <none>           <none>
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          98m     <none>            node2    <none>           <none>
此时，master运行到了其他的node节点上，故目录下没有相应的文件，删掉，指定运行在master主机上即可
[root@master manifests]# kubectl delete endpoints -n kube-system kube-controller-manager
endpoints "kube-controller-manager" deleted
指定address地址
[root@master manifests]# pwd
/etc/kubernetes/manifests
[root@master manifests]# cat kube-controller-manager.yaml  | grep -1 address
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
--
    - --leader-elect=false
    - --address=127.0.0.1
    - --node-cidr-mask-size=24
删除这个pod
[root@master manifests]# kubectl delete -f kube-controller-manager.yaml 
pod "kube-controller-manager" deleted
再启动这个pod
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
检查仍然找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master manifests]# kubectl logs kube-controller-manager -n kube-system
I0822 11:57:57.489290       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory
查看kube-apiserver-master 还是运行在node2节点上？？？
[root@master manifests]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS              RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATES
coredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          5h4m    <none>            master   <none>           <none>
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          5h4m    <none>            master   <none>           <none>
etcd-master                             1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-apiserver-master                   1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-controller-manager                 0/1     CrashLoopBackOff    3          83s     192.168.189.202   node2    <none>           <none>
kube-controller-manager-master          1/1     Running             0          6m4s    192.168.189.200   master   <none>           <none>
kube-proxy-dzr8s                        1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-proxy-vkqzq                        1/1     Running             0          3h44m   192.168.189.201   node1    <none>           <none>
kube-proxy-vrw77                        1/1     Running             0          3h44m   192.168.189.202   node2    <none>           <none>
kube-scheduler                          1/1     Running             0          16m     192.168.189.201   node1    <none>           <none>
kube-scheduler-master                   1/1     Running             0          22m     192.168.189.200   master   <none>           <none>
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          176m    <none>            node2    <none>           <none>
分析kube-controller-manager启动参数，leader-elect设置为true，此为高可用场景下多个kube-controller-manager实例竞争选举哪个实例为leader角色的开关，开启
时kube-controller-manger实例启动时会连接kube-api竞争创建名为kube-controller-manager的endpoint，创建成功的kube-controller-manger实例为leader，其他
实例为backup，同时leader实例需要定期更新此endpoint，维持leader地位。
此环境为非高可用环境，修改leader-elect为false避免kube-controller-manager定期去连接kube-api更新endpoint，理论也可以避免renew超时退出问题
[root@master manifests]# cat kube-scheduler.yaml | grep leader
    - --leader-elect=true
[root@master manifests]# cat kube-controller-manager.yaml | grep leader
    - --leader-elect=false

明天试验
1、增大master节点的内存，使用top命令查看master内存只剩余100多m，但是配置文件中原本为200m，改为100m仍然不行，增加内存再试
[root@master manifests]# cat kube-controller-manager.yaml | grep -i cpu
        cpu: 100m
[root@master manifests]# cat kube-scheduler.yaml| grep -i cpu
        cpu: 100m
2、controller-manager和scheduler都为pod，将pod绑定在master节点，master需要增加内存
此处port=0注释掉，忘记为什么注释明天查一下
[root@master manifests]# cat kube-controller-manager.yaml | grep port
#    - --port=0
        port: 10257
[root@master manifests]# cat kube-scheduler.yaml | grep port
#    - --port=0
        port: 10259

[root@master manifests]# kubectl logs kube-controller-manager -n kube-system
Flag --address has been deprecated, see --bind-address instead.
I0822 11:50:40.362080       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory

查看kubelet的cgroup
[root@master kubelet.service.d]# systemctl show --property=Enviroment kubelet | grep -i cgroup
