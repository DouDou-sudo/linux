3台机器配置为2G2核
主机名   操作系统版本     ip           docker version   kubeadm/kubelet/kubectl version
master    7.9.2009  192.168.189.200                        1.23.9
node1     7.9.2009  192.168.189.201                        1.23.9
node2     7.9.2009  192.168.189.202                        1.23.9
一、基础环境准备
1、修改主机名
[root@localhost ~]# hostnamectl set-hostname master
[root@localhost ~]# su
[root@master ~]#

2、关闭防火墙
[root@master ~]# setenforce 0
[root@master ~]# sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
[root@master ~]# systemctl disable --now firewalld

3、禁用swap
[root@master ~]# swapoff -a
[root@master ~]# sed -i.bak '/swap/s/^/#/g' /etc/fstab

4、配置dns
[root@master ~]# echo "nameserver 223.5.5.5" >> /etc/resolv.conf

5、配置yun源
[root@master ~]# curl -o /etc/yum.repos.d/Centos-7.repo   https://mirrors.aliyun.com/repo/Centos-7.repo
[root@master ~]# curl -o /etc/yum.repos.d/epel-7.repo   https://mirrors.aliyun.com/repo/epel-7.repo
[root@master ~]# curl -o /etc/yum.repos.d/docker-ce.repo   https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@master yum.repos.d]# cat > kubernetes.repo  <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
[root@master yum.repos.d]# yum clean all && yum makecache
初次yum makecache时在生成centos源缓存时报错
base/7/x86_64/filelists_db     FAILED                                          
http://mirrors.aliyuncs.com/centos/7/os/x86_64/repodata/d6d94c7d406fe7ad4902a97104b39a0d8299451832a97f31d71653ba982c955b-filelists.sqlite.bz2:
 [Errno 14] curl#7 - "Failed connect to mirrors.aliyuncs.com:80; Connection refused"Trying other mirror.
查看Centos-7.repo文件为http，修改为https后再次执行makecahce正常
[root@master yum.repos.d]# sed -i.bak 's/http/https/g' Centos-7.repo
[root@master yum.repos.d]# yum clean all && yum makecache

6、下载常见安装包
[root@master ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 wegt net-tools vim sshpass

7、修改hosts文件
[root@master ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.189.200 master
192.168.189.201 node1
192.168.189.202 node2
[root@master ~]# scp -rp /etc/hosts node1:/etc/                                                                                                    100%  203    59.0KB/s   00:00    
[root@master ~]# scp -rp /etc/hosts node2:/etc/

8、配置ssh互信
[root@master ~]# ssh-keygen 按两下空格
[root@master ~]# cat believe.sh
#!/bin/bash
pass=root.2020
awk 'NR>2{print $2}' /etc/hosts  |while read line
do
	sshpass -p $pass ssh-copy-id -o stricthostkeychecking=no $line
done
[root@master ~]# chmod +x believe.sh 
[root@master ~]# ./believe.sh 

9、配置时间同步
修改时区
[root@master ~]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
[root@master ~]# timedatectl set-timezone 'Asia/Shanghai'
修改chrony.conf文件
[root@master ~]# grep -Ev "^#|^$" /etc/chrony.conf
server cn.pool.ntp.org 
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.168.189.0/24
local stratum 10
logdir /var/log/chrony
拷贝给node节点
[root@master ~]# scp -rp /etc/chrony.conf node1:/etc/
chrony.conf                                                                                                 100% 1183   356.1KB/s   00:00    
[root@master ~]# scp -rp /etc/chrony.conf node2:/etc/
chrony.conf                                                                                                 100% 1183   373.9KB/s   00:00  
设置开机自启并启动chronyd服务，查看Time zone和NTP synchronized
[root@master ~]# systemctl enable --now chronyd
查看时区和时间同步
[root@master ~]# timedatectl 
      Local time: Mon 2022-08-22 10:05:27 CST
  Universal time: Mon 2022-08-22 02:05:27 UTC
        RTC time: Mon 2022-08-22 02:05:27
       Time zone: Asia/Shanghai (CST, +0800)
     NTP enabled: yes
NTP synchronized: yes
 RTC in local TZ: no
      DST active: n/a
同步到硬件时间
[root@master ~]# hwclock -w

10、配置内核参数
[root@master ~]# cat >>/etc/sysctl.conf <<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
vm.swappiness=0
EOF
[root@master ~]# sysctl -p
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
使用modprobe加载br_netfilter模块
[root@master ~]# modprobe br_netfilter
[root@master ~]# sysctl -p
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
vm.swappiness=0

11、安装docker-ce
使用如下命令查看docker版本，本次选择20.10.10版
[root@master ~]# yum list docker-ce --showduplicates
[root@master ~]# yum install -y docker-ce-20.10.10
[root@master ~]# systemctl enable --now docker
镜像加速，修改cgroup为systemd
[root@master ~]# cat >>/etc/docker/daemon.json <<EOF
{
  "registry-mirrors": ["https://v16stybc.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
[root@master ~]# systemctl daemon-reload
[root@master ~]# systemctl restart docker
使用如下命令查看Registry Mirrors和Cgroup Driver是否修改成功
[root@master ~]# docker info | grep -in1  reg
47- Debug Mode: false
48: Registry: https://index.docker.io/v1/
49- Labels:
50- Experimental: false
51: Insecure Registries:
52-  127.0.0.0/8
53: Registry Mirrors:
54-  https://u96790vf.mirror.aliyuncs.com/
[root@master ~]# docker info | grep -i dri
 Storage Driver: overlay2
 Logging Driver: json-file
 Cgroup Driver: systemd
使用docker运行hello-world测试
[root@master ~]# docker run hello-world

12、安装kubelet、kubeadm、kubectl
[root@master ~]# yum list kubelet --showduplicates
[root@master ~]# yum install -y kubeadm-1.17.12 kubectl-1.17.12 kubelet-1.17.12
[root@master ~]# systemctl enable --now kubelet
[root@master ~]# systemctl status kubelet ##此时查看kubelet频繁重启属于正常现象，kubeadm init后就会启动
[root@master ~]# ss -tunlp | grep kubelet ##端口没有放开也属于正常现象，kubeadm init后端口会放开

13、kubectl命令补全
[root@master ~]# yum install -y bash-completion
[root@master ~]# source /etc/profile.d/bash_completion.sh 
[root@master ~]# echo "source <(kubectl completion bash)" >>~/.bash_profile 
[root@master ~]# source ~/.bash_profile

14、下载镜像
[root@master ~]# cat images.sh 
#!/bin/bash
url=registry.cn-hangzhou.aliyuncs.com/google_containers
version=v1.17.12
images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
for imagename in ${images[@]} ; do
  docker pull $url/$imagename
  docker tag $url/$imagename k8s.gcr.io/$imagename
  docker rmi -f $url/$imagename     
done
[root@master ~]# chmod +x images.sh
[root@master ~]# ./images.sh 

15、kubeadm init
初始化可以使用两种方式，一种如下，还有一种创建配置文件，配置更详细kubeadm init --config kubeadm-config.yaml
cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1alpha2 
kind: MasterConfiguration 
kubernetesVersion: v1.17.2 
api:
    advertiseAddress: 192.168.189.200
    bindPort: 6443 
    controlPlaneEndpoint: "" 
imageRepository: k8s.gcr.io 
kubeProxy: 
    config: 
    mode: "ipvs"     
    ipvs: 
        ExcludeCIDRs: null 
        minSyncPeriod: 0s 
        scheduler: "" 
        syncPeriod: 30s 
kubeletConfiguration: 
    baseConfig: 
        cgroupDriver: systemd
        clusterDNS: 
        -10.96.0.10 
        clusterDomain: cluster.local 
        failSwapOn: false 
        resolvConf: /etc/resolv.conf 
        staticPodPath: /etc/kubernetes/manifests 
networking: 
    dnsDomain: cluster.local 
    podSubnet: 10.1.200.0/24
    serviceSubnet: 10.1.100.0/24
[root@master ~]# kubeadm init --apiserver-advertise-address=192.168.189.200 --image-repository registry.aliyuncs.com/google_containers --kuber
netes-version v1.17.12 --service-cidr=10.1.100.0/24 --pod-network-cidr=10.1.200.0/24
输出如下表示初始化成功
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.189.200:6443 --token yquoub.htqg9ff5za70wxa4 \
    --discovery-token-ca-cert-hash sha256:1897ee409743739e39d713e9da8ea99571eb578bbd163046fbf2b882f99f6e0d 
此时查看kubelet状态和端口开放情况
[root@master ~]# ss -tunlp | grep kubelet
tcp    LISTEN     0      128    127.0.0.1:40820                 *:*                   users:(("kubelet",pid=27544,fd=8))
tcp    LISTEN     0      128    127.0.0.1:10248                 *:*                   users:(("kubelet",pid=27544,fd=27))
tcp    LISTEN     0      128    [::]:10250              [::]:*                   users:(("kubelet",pid=27544,fd=24))
[root@master ~]# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Mon 2022-08-22 14:54:10 CST; 16min ago

16、加载环境变量
使用普通用户执行如下命令
[root@kube ~]# mkdir -p $HOME/.kube
[root@kube ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@kube ~]# chown $(id -u):$(id -g) $HOME/.kube/config
我使用的是root用户，只需要.bash_profile中添加环境变量就行
[root@master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >>~/.bash_profile 
[root@master ~]# source ~/.bash_profile

17、安装pod网络，本次使用的是flannel
[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
[root@master ~]# kubectl apply -f  kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
安装完pod网络后要等到几分钟，执行这个命令查看直到都为running状态
[root@master ~]# kubectl get pods -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
coredns-6955765f44-7z8cc         1/1     Running   0          11m
coredns-6955765f44-dfxsc         1/1     Running   0          11m
etcd-master                      1/1     Running   0          11m
kube-apiserver-master            1/1     Running   0          11m
kube-controller-manager-master   1/1     Running   0          11m
kube-proxy-9ht4l                 1/1     Running   0          11m
kube-scheduler-master            1/1     Running   0          11m

如果需要重新安装先删除所创建的网络配置
kubectl delete -f  kube-flannel.yml
All
18、work节点加入集群
[root@node2 yum.repos.d]# yum install -y kubeadm-1.17.12 kubectl-1.17.12 kubelet-1.17.12
[root@node2 yum.repos.d]# yum install -y bash-completion
[root@node2 yum.repos.d]# source /etc/profile.d/bash_completion.sh 
[root@node2 yum.repos.d]# echo "source <(kubectl completion bash)" >>~/.bash_profile 
[root@node2 yum.repos.d]# systemctl enable --now kubelet ##此时查看kubelet频繁重启属于正常现象，kubeadm join后就会启动
[root@node1 ~]# kubeadm join 192.168.189.200:6443 --token yquoub.htqg9ff5za70wxa4 \
>     --discovery-token-ca-cert-hash sha256:1897ee409743739e39d713e9da8ea99571eb578bbd163046fbf2b882f99f6e0d 

19、查看node
[root@master manifests]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   24h   v1.17.12
node1    Ready    <none>   23h   v1.17.12
node2    Ready    <none>   23h   v1.17.12

20、健康检查发现controller-manager、scheduler状态Unhealthy，查看master组件状态时出现错误
[root@master ~]# kubectl get cs
NAME                 STATUS      MESSAGE                                                                                     ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"}  
修改/etc/kubernetes/manifests/kube-controller-manager.yaml、/etc/kubernetes/manifests/kube-scheduler.yaml文件
[root@master manifests]# cd /etc/kubernetes/manifests
[root@master manifests]# ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
[root@master manifests]# cat kube-controller-manager.yaml | grep port
    - --port=10252
        port: 10257
[root@master manifests]# cat kube-scheduler.yaml | grep port
    - --port=10251
        port: 10259
修改完成后，重新apply即可，再次检查健康状态
[root@master manifests]# kubectl apply -f kube-scheduler.yaml 
pod/kube-scheduler created
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
[root@master manifests]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
[root@master manifests]# 

遇到的问题
使用如下命令查看kube-controller-manager状态为CrashLoopBackOff 
[root@master ~]# kubectl get pods -n kube-system
NAME                                    READY   STATUS              RESTARTS   AGE
coredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          3h41m
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          3h41m
etcd-master                             1/1     Running             1          3h41m
kube-apiserver-master                   1/1     Running             1          3h41m
kube-controller-manager                 0/1     CrashLoopBackOff    12         38m
kube-controller-manager-master          1/1     Running             0          42m
kube-proxy-dzr8s                        1/1     Running             1          3h41m
kube-proxy-vkqzq                        1/1     Running             0          141m
kube-proxy-vrw77                        1/1     Running             0          141m
kube-scheduler                          1/1     Running             0          38m
kube-scheduler-master                   1/1     Running             0          41m
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          93m
使用如下命令查看日志报错，找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master ~]# kubectl logs -n kube-system kube-controller-manager
I0822 10:34:23.104970       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory

使用如下命令查看kube-controller-manager运行在了node节点
[root@master ~]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS              RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS G
ATEScoredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          3h46m   <none>            master   <none>           <none>
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          3h46m   <none>            master   <none>           <none>
etcd-master                             1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-apiserver-master                   1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-controller-manager                 0/1     CrashLoopBackOff    13         43m     192.168.189.202   node2    <none>           <none>
kube-controller-manager-master          1/1     Running             0          47m     192.168.189.200   master   <none>           <none>
kube-proxy-dzr8s                        1/1     Running             1          3h46m   192.168.189.200   master   <none>           <none>
kube-proxy-vkqzq                        1/1     Running             0          147m    192.168.189.201   node1    <none>           <none>
kube-proxy-vrw77                        1/1     Running             0          146m    192.168.189.202   node2    <none>           <none>
kube-scheduler                          1/1     Running             0          44m     192.168.189.201   node1    <none>           <none>
kube-scheduler-master                   1/1     Running             0          46m     192.168.189.200   master   <none>           <none>
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          98m     <none>            node2    <none>           <none>
此时，master运行到了其他的node节点上，故目录下没有相应的文件，删掉，指定运行在master主机上即可
[root@master manifests]# kubectl delete endpoints -n kube-system kube-controller-manager
endpoints "kube-controller-manager" deleted
指定address地址
[root@master manifests]# pwd
/etc/kubernetes/manifests
[root@master manifests]# cat kube-controller-manager.yaml  | grep -1 address
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
--
    - --leader-elect=false
    - --address=127.0.0.1
    - --node-cidr-mask-size=24
删除这个pod
[root@master manifests]# kubectl delete -f kube-controller-manager.yaml 
pod "kube-controller-manager" deleted
再启动这个pod
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
检查仍然找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master manifests]# kubectl logs kube-controller-manager -n kube-system
I0822 11:57:57.489290       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory
查看kube-apiserver-master 还是运行在node2节点上？？？
[root@master manifests]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS              RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATES
coredns-9d85f5447-l2ncb                 0/1     ContainerCreating   0          5h4m    <none>            master   <none>           <none>
coredns-9d85f5447-nh4np                 0/1     ContainerCreating   0          5h4m    <none>            master   <none>           <none>
etcd-master                             1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-apiserver-master                   1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-controller-manager                 0/1     CrashLoopBackOff    3          83s     192.168.189.202   node2    <none>           <none>
kube-controller-manager-master          1/1     Running             0          6m4s    192.168.189.200   master   <none>           <none>
kube-proxy-dzr8s                        1/1     Running             2          5h4m    192.168.189.200   master   <none>           <none>
kube-proxy-vkqzq                        1/1     Running             0          3h44m   192.168.189.201   node1    <none>           <none>
kube-proxy-vrw77                        1/1     Running             0          3h44m   192.168.189.202   node2    <none>           <none>
kube-scheduler                          1/1     Running             0          16m     192.168.189.201   node1    <none>           <none>
kube-scheduler-master                   1/1     Running             0          22m     192.168.189.200   master   <none>           <none>
kubernetes-dashboard-778ff9499c-2dhj2   0/1     ContainerCreating   0          176m    <none>            node2    <none>           <none>
分析kube-controller-manager启动参数，leader-elect设置为true，此为高可用场景下多个kube-controller-manager实例竞争选举哪个实例为leader角色的开关，开启
时kube-controller-manger实例启动时会连接kube-api竞争创建名为kube-controller-manager的endpoint，创建成功的kube-controller-manger实例为leader，其他
实例为backup，同时leader实例需要定期更新此endpoint，维持leader地位。
此环境为非高可用环境，修改leader-elect为false避免kube-controller-manager定期去连接kube-api更新endpoint，理论也可以避免renew超时退出问题
[root@master manifests]# cat kube-scheduler.yaml | grep leader
    - --leader-elect=true
[root@master manifests]# cat kube-controller-manager.yaml | grep leader
    - --leader-elect=false

[root@master manifests]# kubectl logs kube-controller-manager -n kube-system
Flag --address has been deprecated, see --bind-address instead.
I0822 11:50:40.362080       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory

查看kubelet的cgroup
[root@master kubelet.service.d]# systemctl show --property=Enviroment kubelet | grep -i cgroup


22、重新kubeadm init
master节点
使用kubeadm reset命令输入y
The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
提示/etc/cni/net.d/、iptables、ipvs、$HOME/.kube/config file需要手动清理，
[root@master ~]# rm -rf .kube/
[root@master ~]# rm -rf /etc/cni/net.d/
[root@master ~]# iptables -F

node节点加入
[root@node1 manifests]# kubeadm join 192.168.189.200:6443 --token j1omv1.zsechs2qoyawqkgy \
>     --discovery-token-ca-cert-hash sha256:5c205b9adf74cb147494ceb56fefaba24749a1827d6d9e9360184b91568fd553 
-bash: rpkubeadm: command not found
[root@node1 manifests]# kubeadm join 192.168.189.200:6443 --token j1omv1.zsechs2qoyawqkgy --discovery-token-ca-cert-hash sha256:5c205b9adf74cb147494ceb56fefaba24749a1827d6
d9e9360184b91568fd553W0823 16:45:27.941697    8953 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.10. Latest validated version: 19.03
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@node1 ~]# kubeadm  reset  提示输入：y
The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.

[root@node1 ~]# rm -rf  /etc/cni/net.d/
[root@node1 ~]# iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
[root@node1 ~]# kubeadm join 192.168.189.200:6443 --token j1omv1.zsechs2qoyawqkgy --discovery-token-ca-cert-hash sha256:5c205b9adf74cb147494ceb56fefaba24749a1827d6d9e93601
84b91568fd553W0823 17:16:51.319915   19190 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.10. Latest validated version: 19.03
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

#[root@node2 ~]# rm -rf  /etc/kubernetes/kubelet.conf
#[root@node2 ~]# rm -rf /etc/kubernetes/pki/ca.crt
#[root@node2 ~]# rm -rf /etc/kubernetes/bootstrap-kubelet.conf
#[root@node2 ~]# iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
主节点上查看
[root@master ~]# kubectl get nodes
NAME     STATUS     ROLES    AGE    VERSION
master   Ready      master   58m    v1.17.12
node1    NotReady   <none>   9m6s   v1.17.12
node2    NotReady   <none>   92s    v1.17.12

23、开启kube-proxy开启ipvs的前置条件
[root@master ~]# cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

加载模块
[root@master ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

安装了ipset软件包
[root@master ~]# yum install ipset -y

安装管理工具ipvsadm
[root@master ~]# yum install ipvsadm -y

docker批量删除images
docker rmi `docker images | awk 'NR>1{print $1":"$2'}`

24、k8s升级
在 master 节点执行
kubeadm config view
创建kubeadm-config-upgrade.yaml配置文件，文件内容如下，根据前面 kubeadm config view 的执行结果，修改了如下字段：
imageRepository 的值修改为：registry.aliyuncs.com/google_containers
#kubernetesVersion 的值修改为： v1.20.7

apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.20.7
networking:
  dnsDomain: cluster.local
  podSubnet: 10.44.0.0/16
  serviceSubnet: 10.22.0.0/16
scheduler: {}

# 查看配置文件差异
kubeadm upgrade diff --config kubeadm-config-upgrade.yaml
     
# 执行升级前试运行
kubeadm upgrade apply --config kubeadm-config-upgrade.yaml --dry-run
     
# 执行升级动作
kubeadm upgrade apply --config kubeadm-config-upgrade.yaml

加载ipvs模块
kubernete 开启ipvs 模式，主要修改kube-proxy 配置
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
lsmod | grep ip_vs
lsmod | grep nf_conntrack_ipv4
yum install -y ipvsadm ipset
# 开启IPVS，修改ConfigMap的kube-system/kube-proxy中的模式为ipvs
kubectl edit cm kube-proxy -n kube-system 
# 将空的data -> ipvs -> mode中替换如下
mode: "ipvs"

使用[root@master ~]# kubectl get pods --all-namespaces查看所有pod

拉去阿里云镜像
docker pull registry.cn-beijing.aliyuncs.com/yunweijia/flannel:v0.12.0-arm64 
docker tag registry.cn-beijing.aliyuncs.com/yunweijia/flannel:v0.12.0-arm64   quay.io/coreos/flannel:v0.12.0-arm64 
