污点(Taint)则相反它使节点能够排斥一类特定的 Pod。
容忍度（Toleration）是应用于 Pod 上的，允许（但并不要求）Pod 调度到带有与之匹配的污点的节点上。
设计理念：Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调度到具有污点的特殊配置的节点上

容忍与nodeSelector区别

污点在k8s中相当于给node设置了一个锁，容忍相当于这个锁的钥匙，调度到这个节点的Pod需要使用容忍来解开这个锁，但是污点与容忍并不会固定的把某些Pod调度到这个节点。如果Pod第一次调度被调度到了没有污点的节点，他就会被分配到这个节点，容忍的配置就无效了，如果这个Pod正好被调度到有污点的node，他的容忍配置会被解析，验证这个Pod是否可以容忍node的污点。

nodeSelector固定调度某些Pod到指定的一些节点，他的作用是强制性的，如果集群中没有符合的node，Pod会一直处于等待调度的阶段。

1、nodeSelector
强制指定pod运行在某些节点
1.1 查看节点的labels
[root@k8smaster volume]# kubectl get nodes  --show-labels
NAME        STATUS   ROLES    AGE   VERSION   LABELS
k8smaster   Ready    master   20d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cm=test,kubernetes.io
/arch=amd64,kubernetes.io/hostname=k8smaster,kubernetes.io/os=linux,node-role.kubernetes.io/master=
k8snode1    Ready    <none>   20d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=am
d64,kubernetes.io/hostname=k8snode1,kubernetes.io/os=linux
k8snode2    Ready    <none>   20d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=am
d64,kubernetes.io/hostname=k8snode2,kubernetes.io/os=linux

1.2 增加labels
[root@k8smaster volume]# kubectl label nodes k8smaster drc=test
node/k8smaster labeled
1.3 删除labels
[root@k8smaster volume]# kubectl label nodes k8smaster drc-
node/k8smaster labeled

1.4 示例
给k8smaster节点打上cm=test的labels
[root@k8smaster nodeSelector]# kubectl get nodes k8smaster --show-labels
NAME        STATUS   ROLES    AGE   VERSION   LABELS
k8smaster   Ready    master   20d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cm=test,kubernetes.io
/arch=amd64,kubernetes.io/hostname=k8smaster,kubernetes.io/os=linux,node-role.kubernetes.io/master=
指定pod运行在携带cm=test 标签的节点上
[root@k8smaster nodeSelector]# cat nodeSe-pod.yml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: busybox-pod
  name: test-busybox
spec:
  containers:
  - command:
    - sleep
    - "2400"
    image: busybox
    imagePullPolicy: Always
    name: test-busybox
  nodeSelector:
    cm: test
查看是否允许在k8smaster节点上
[root@k8smaster nodeSelector]# kubectl get pods -o wide
test-busybox                        1/1     Running   0          18s     10.244.0.4    k8smaster   <none> 
当pod已经运行在节点上，删除掉节点上的labels，不影响已经运行的pod

2、Taint和Toleration
相当于给node设置了一个锁，容忍相当于这个锁的钥匙，调度到这个节点的Pod需要使用容忍来解开这个锁，但是污点与容忍并不会固定的把某些Pod调度到这个节点。如果Pod第一次调度被调度到了没有污点的节点，他就会被分配到这个节点，容忍的配置就无效了，如果这个Pod正好被调度到有污点的node，他的容忍配置会被解析，验证这个Pod是否可以容忍node的污点。
打上Taint的node，除非pod配置了Toleration这个Taint，否则不会调度到该node上
2.1 Taint
2.1.1 查看污点
[root@k8smaster nodeSelector]# kubectl get nodes k8smaster -o go-template --template {{.spec.taints}}
<no value>[root@k8smaster nodeSelector]# 
[root@k8smaster nodeSelector]# kubectl describe nodes k8smaster | grep  -10  Ta
Taints:             <none>

2.1.2 添加污点
kubectl taint nodes node名称 key=value:那种污点
污点配置详解

    NoSchedule：禁止调度到该节点，已经在该节点上的Pod不受影响
    NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）
    PreferNoSchedule：尽量避免将Pod调度到指定的节点上，如果没有更合适的节点，可以部署到该节点

[root@k8smaster nodeSelector]# kubectl taint node k8snode1 ssd=true:NoExecute
node/k8snode1 tainted
[root@k8smaster nodeSelector]# kubectl describe nodes k8snode1 | grep Ta
Taints:             ssd=true:NoExecute

2.1.3 污点删除
[root@k8smaster ~]# kubectl taint node k8snode1 ssd=true:NoExecute-
node/k8snode1 untainted
[root@k8smaster ~]# kubectl taint node k8snode1 ssd-
node/k8snode1 untainted

2.1.4 k8s内置污点
    node.kubernetes.io/not-ready：节点未准备好，相当于节点状态Ready的值为False。
    node.kubernetes.io/unreachable：Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。node.kubernetes.io/out-of-disk：节点磁盘耗尽。
    node.kubernetes.io/memory-pressure：节点存在内存压力。
    node.kubernetes.io/disk-pressure：节点存在磁盘压力。
    node.kubernetes.io/network-unavailable：节点网络不可达。
    node.kubernetes.io/unschedulable：节点不可调度。
    node.cloudprovider.kubernetes.io/uninitialized：如果Kubelet启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个controller初始化这个节点后，Kubelet将删除这个Taint。

2.2 容忍Toleration
容忍作用与Pod资源配置在tolerations字段，用于容忍配置在node节点的Taint污点，可以让Pod部署在有污点的node节点。\

k8s中部署pod会自动添加一些内置的容忍，容忍的默认时间的300秒，表示在node节点出现了内置污点后，会容忍3000秒的时间之后会进行迁移
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
查看kube-proxy的Tolerations
[root@k8smaster ~]# kubectl describe pod -n kube-system kube-proxy-9mspw
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule

2.3 示例
给k8snode1打上污点ssd=node2:NoSchedule
[root@k8smaster tolerations]# kubectl describe nodes k8snode1 | grep Ta
Taints:             ssd=node2:NoSchedule
给k8snode1打上labels ssh=true
[root@k8smaster tolerations]# kubectl get nodes k8snode1 --show-labels
NAME       STATUS   ROLES    AGE   VERSION   LABELS
k8snode1   Ready    <none>   21d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kuber
netes.io/hostname=k8snode1,kubernetes.io/os=linux,ssh=true
创建Deployment，容忍污点key: "ssd"的所有污点，指定运行在有ssh: "true" 标签的node上
[root@k8smaster tolerations]# cat toleration-pod.yml 
apiVersion: apps/v1    
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      tolerations:   #容忍配置
      - key: "ssd"
        operator: "Exists"
      containers:
      - name: nginx
        image: nginx:1.8
      nodeSelector:  #指定调度的节点
        ssh: "true
查看pod运行在携带key: "ssd"污点的k8snode1上
[root@k8smaster tolerations]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
nginx-7cc999dbd-xgw4l               1/1     Running   0          5m30s   10.244.1.61   k8snode1    <none> 
"
2.4 容忍匹配
方式一：完全匹配

tolerations:
- key: "taintKey"      #污点的key名称
  operator: "Equal"    #匹配类型，Equal表示匹配污点的所有值
  value: "taintValue"  #污点key的值
  effect: "NoSchedule" #污点类型
此时taint和toleration的k，v，effect要完全相等，任意一个不匹配就不能容忍
方式二：不完全匹配

tolerations:
- key: "taintKey"      #污点的key值
  operator: "Exists"   #匹配类型，只要符合污点设置的值即可，配置那些就表示只匹配那些
  effect: "NoSchedule" #污点的类型
#注意不完全匹配，设置可以是一个可以是俩个，有自己定义比如
此时taint和toleration的只匹配k，effect

方式三：大范围匹配（不推荐key为内置Taint）

tolerations:
- key: "taintKey"      #污点的key值
  operator: "Exists"   #匹配类型，只要符合污点设置的值即可，配置那些就表示只匹配那些
此时只匹配key

方式四：匹配所有（不推荐）

表示匹配所有污点，不管什么污点都匹配。在k8s中的daemonsets资源默认情况下是容忍所有污点的

tolerations:
- operator: "Exists"  #匹配所有，没有任何条件
#Daemonset资源默认容忍，kube-proxy的tolerations 也包括 operator: Exists
kubectl  get daemonsets.apps -n kube-system cilium -oyaml | grep tolerations -A 10
      tolerations:
      - operator: Exists

空的 effect 匹配所有的 effect
其他配置，停留时间配置

如果在容忍中添加tolerationSeconds参数，表示这个容忍只容忍一定的时间，之后会进行迁移。

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 360  #容忍时间

3、 Affinity

调度器在使用的时候，经过了 predicates 和 priorities 两个阶段，
但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 Pod 的调度，
亲和力作用于俩方面，分别是Pod与node之间的关系，Pod与Pod之间的关系。

    NodeAffinity：节点亲和力/反亲和力
    PodAffinity：Pod亲和力
    PodAntiAffinity：Pod反亲和力

亲和性调度可以分成软策略和硬策略两种方式:
   
   1、软策略就是如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓
   2、硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然就不干了
对于亲和性和反亲和性都有这两种规则可以设置： preferredDuringSchedulingIgnoredDuringExecution 和requiredDuringSchedulingIgnoredDuringExecution，前面的就是软策略，后面的就是硬策略。

3.1 nodeAffinity

[root@k8smaster affinity]# cat affinity-deploy.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-affinity
  labels:
    app: node-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: node-affinity
  template:
    metadata:
      labels:
        app: node-affinity
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
          name: nginxweb
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
            nodeSelectorTerms:                    #节点选择器配置，只能配置一个
            - matchExpressions:                   #匹配条件设置，可以配置多个，如果是多个他们是或的关系，所有条件都可以被匹配
              - key: kubernetes.io/hostname       #node的labels，匹配的node的key设置,可以配置多个，如果配置多个key他们的关系为and，即需要满足所有的条件才会被匹配
                operator: NotIn                   #匹配方式，有多种
                values:                           #key值，可以写多个
                - k8snode2
          preferredDuringSchedulingIgnoredDuringExecution:  # 软策略
          - weight: 1                             #软亲和力的权重，权重越高优先级越大，范围1-100
            preference:                           #软亲和力配置项，和weight同级
              matchExpressions:                   #匹配条件设置
              - key: cm                           #匹配的node的key设置,与硬亲和力一致
                operator: In
                values:
                - test
这个pod不能运行在k8snode2这个节点上，尽量运行在携带cm=test的labels的node上，有Taint的node不能调度
查看node的labels，k8snode1携带了cm=test的labels，kubernetes.io/hostname的labels每个node默认携带
[root@k8smaster affinity]# kubectl get nodes  --show-labels
NAME        STATUS   ROLES    AGE   VERSION   LABELS
k8smaster   Ready    master   21d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kube
rnetes.io/hostname=k8smaster,kubernetes.io/os=linux,node-role.kubernetes.io/master=
k8snode1    Ready    <none>   21d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cm=test,kubernetes.io/arch=am
d64,kubernetes.io/hostname=k8snode1,kubernetes.io/os=linux
k8snode2    Ready    <none>   21d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kube
rnetes.io/hostname=k8snode2,kubernetes.io/os=linux
k8snode1携带了cm=test的labels，可以看到全部调度到了k8snode1节点上
[root@k8smaster affinity]# kubectl get pods -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP            NODE        NOMINATED NODE   READINESS GATES
node-affinity-84d4dc5cc5-29lzj   1/1     Running   0          9s    10.244.1.69   k8snode1    <none>           <none>
node-affinity-84d4dc5cc5-7jdl2   1/1     Running   0          9s    10.244.1.71   k8snode1    <none>           <none>
node-affinity-84d4dc5cc5-ktbqg   1/1     Running   0          9s    10.244.1.70   k8snode1    <none>           <none>

这里的匹配逻辑是 label 标签的值在某个列表中，现在 Kubernetes 提供的操作符有下面的几种：

In:label 的值在某个列表中,相当于key = value的形式
NotIn:label 的值不在某个列表中,相当于key != value的形式
Gt:label 的值大于某个值,大于value指定的值
Lt:label 的值小于某个值,小于value指定的值
Exists:节点存在label的key为指定的值即可,不能配置values字段
DoesNotExist:节点不存在label的key为指定的值即可,不能配置values字段
但是需要注意的是如果 nodeSelectorTerms 下面有多个选项的话，满足任何一个条件就可以了；
如果 matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod。

3.2 podAffinity
Pod 亲和性（podAffinity）主要解决 Pod 可以和哪些 Pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 Pod 反亲和性主要是解决 Pod 不能和哪些 Pod 部署在同一个拓扑域中的问题，它们都是处理的 Pod 与 Pod 之间的关系，比如一个 Pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 Pod 在节点上了，那么我就不想和你待在同一个节点上。

3.2.1  Pod亲和力和反亲和力详解
spec:
      affinity:
        podAntiAffinity:   #Pod反亲和配置与亲和配置一致，可以与Pod亲和一起存在
        podAffinity:       #Pod亲和配置，可以与Pod反亲和一起存在
          requiredDuringSchedulingIgnoredDuringExecution:  #硬亲和，与软亲和只能选择其中一种，硬亲和如果找不到匹配的kv会一直处于Pending状态
          - labelSelector:               #Pod标签选择器
              matchExpressions:   #和节点亲和力配置一致，只能配置1个
              - key: name         #pod的标签设置可以设置多个，多个key必须存在于1个Pod
                operator: In      #配置和节点亲和力一致，但是没有Gt和Lt
                values:
                - "blackbox"
                - "blackbox-1"
            namespaces:     # 和哪个命名空间的Pod进行匹配，为空为当前命名空间
            - helm
            topologyKey: kubernetes.io/hostname  #匹配的拓扑域的key，也就是节点上label的key，key和value相同的为同一个域，可以用于标注不同的机房和地区
          preferredDuringSchedulingIgnoredDuringExecution:  #软亲和，与硬亲和只能选择其中一种
          - weight: 100    #权重1-100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: security
                  operator: In
                  values:
                  - S2
              namespaces:
              - default
              topologyKey: failure-domain.beta.kubernetes.io/zone
示例
[root@k8smaster affinity]# cat pod-affinity.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-affinity
  labels:
    app: pod-affinity
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-affinity
  template:
    metadata:
      labels:
        app: pod-affinity
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
          name: nginxweb
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - busybox-pod
            topologyKey: kubernetes.io/hostname
查看携带app=busybox-pod的labels的pod运行在哪个node上
[root@k8smaster affinity]# kubectl get pods -l app=busybox-pod -owide
NAME           READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
test-busybox   1/1     Running   1          15h   10.244.1.68   k8snode1   <none>           <none>
[root@k8smaster affinity]# kubectl apply -f pod-affinity.yml 
deployment.apps/pod-affinity created
可以看到这个pod和携带app=busybox-pod的pod运行在同一个node上
[root@k8smaster affinity]# kubectl get pods -owide
NAME                             READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
pod-affinity-599f889869-rmg9k    1/1     Running   0          7m20s   10.244.1.72   k8snode1    <none>           <none>
test-busybox                     1/1     Running   1          15h     10.244.1.68   k8snode1    <none>           <none>
此时，如果删掉了携带app=busybox-pod的pod,已调度的不影响
[root@k8smaster affinity]# kubectl delete pods test-busybox 
pod "test-busybox" deleted
如果我再删掉已调度的pod,重新调度,因为使用的是硬亲和，pod就会一直处于pending状态,这是因为现在没有一个节点上面拥有 app=busybox-pod 这个标签的 Pod，
[root@k8smaster affinity]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
pod-affinity-599f889869-xcjsq    0/1     Pending   0          109s
再将携带app=busybox-pod的pod启起来，pod-affinity就会运行在同一节点上
[root@k8smaster nodeSelector]# kubectl get pods -owide
NAME                             READY   STATUS              RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS  GATES
pod-affinity-599f889869-xcjsq    1/1     Running             0          5m15s   10.244.2.90   k8snode2    <none>           <none>
test-busybox                     0/1     ContainerCreating   0          11s     <none>        k8snode2    <none>           <none>

3.3 Pod 反亲和性（podAntiAffinity）
一个节点上运行了某个 Pod，那么我们的模板 Pod 则不希望被调度到这个节点上面去了
将上面的 podAffinity 直接改成 podAntiAffinity
[root@k8smaster affinity]# cat pod-affinity.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-affinity
  labels:
    app: pod-affinity
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-affinity
  template:
    metadata:
      labels:
        app: pod-affinity
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
          name: nginxweb
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - busybox-pod
            topologyKey: kubernetes.io/hostname
如果一个节点上面有一个 app=busybox-pod 这样的 Pod 的话，那么我们的 Pod 就别调度到这个节点上面来
查看两个pod运行在了不同的node上
[root@k8smaster affinity]# kubectl get pods -owide
NAME                             READY   STATUS        RESTARTS   AGE    IP            NODE        NOMINATED NODE   READINESS GATES
pod-affinity-784df84b7c-5l2pv    1/1     Running       0          3s     10.244.1.73   k8snode1    <none>           <none>
test-busybox                     1/1     Running       0          35m    10.244.2.91   k8snode2    <none>           <none>


